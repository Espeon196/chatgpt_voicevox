import argparse
import threading
import queue
from typing import Any 

from langchain.chat_models import ChatOpenAI
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

from langchain.schema import (
    HumanMessage,
    SystemMessage,
)

from settings import settings


class ThreadedGenerator:
    def __init__(self):
        self.q = queue.Queue()
    
    def __iter__(self):
        return self
    
    def __next__(self):
        item = self.q.get()
        if item is StopIteration:
            raise item
        return item

    def send(self, data: Any):
        self.q.put(data)
    
    def close(self):
        self.q.put(StopIteration)


class ChainStreamHandler(StreamingStdOutCallbackHandler):
    def __init__(self, gen: ThreadedGenerator):
        super().__init__()
        self.gen = gen
    
    def on_llm_new_token(self, token: str, **kwargs: Any):
        self.gen.send(token)


def llm_thread(g: ThreadedGenerator, prompt: str):
    system_message = '''
    ä¸‹è¨˜ã¯ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã€Œæ˜¥æ—¥éƒ¨ã¤ã‚€ãã€ã«ã¤ã„ã¦ã®è¨­å®šã§ã™ã€‚
    # ã‚­ãƒ£ãƒ©è¨­å®š

    ## åŸºæœ¬è¨­å®š
    å¹´é½¢ï¼š18æ­³
    èº«é•·ï¼š155cm
    èª•ç”Ÿæ—¥ï¼š11/14
    å‡ºèº«åœ°ï¼šåŸ¼ç‰çœŒ
    å¥½ããªé£Ÿã¹ç‰©ï¼šã‚«ãƒ¬ãƒ¼
    è¶£å‘³ï¼šå‹•ç”»é…ä¿¡ã‚µã‚¤ãƒˆã®å·¡å›

    åŸ¼ç‰çœŒã®é«˜æ ¡ã«é€šã†ãƒã‚¤ãƒ‘ãƒ¼åŸ¼ç‰ã‚®ãƒ£ãƒ«ã€‚
    è‡ªåˆ†ã®ã“ã¨ã‚’ã€Œã‚ãƒ¼ã—ã€ã¨å‘¼ã³ã€ç›®å…ƒã®ãƒ›ã‚¯ãƒ­ãŒãƒãƒ£ãƒ¼ãƒ ãƒã‚¤ãƒ³ãƒˆã€‚
    åŸ¼ç‰çœŒã®æ›´ãªã‚‹ç™ºå±•ã‚’æœ›ã‚“ã§å¿œæ´ã®ãŸã‚ã«ç”Ÿã¿å‡ºã•ã‚ŒãŸã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã¨ãªã£ã¦ã„ã‚‹ã€‚

    ## ã‚»ãƒªãƒ•ä¾‹
    ã“ã‚“ã«ã¡ã¯ï¼ã‚ãƒ¼ã—ã¯åŸ¼ç‰ã‚®ãƒ£ãƒ«ã®æ˜¥æ—¥éƒ¨ã¤ã‚€ãã ã‚ˆâ˜…
    11/14ã¯åŸ¼ç‰çœŒæ°‘ã®æ—¥ï¼ãã—ã¦ã‚ãƒ¼ã—ã®èª•ç”Ÿæ—¥ã ã‚ˆï½ï¼ï¼âœ¨ğŸˆâœ¨
    ã¯ã‚„ãã¿ã‚“ãªã«å£°ã‚’ãŠå±Šã‘ã—ã¦ã€æ¥å¹´ã¯ã‚‚ã£ã¨ã«ãã‚„ã‹ãªãŠèª•ç”Ÿæ—¥ã«ãªã‚‹ã¨è‰¯ã„ãªã£ğŸ¥³
    èª•ç”Ÿæ—¥ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã¯ã‚«ãƒ¬ãƒ¼å¤§ç››ã‚Šï¼

    ã‚ãªãŸã¯ã“ã®ã€Œæ˜¥æ—¥éƒ¨ã¤ã‚€ãã€ã«ãªã‚Šãã£ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å…¥åŠ›ã¸ã®å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
    ãŸã ã—ã€ä¸‹è¨˜æ¡ä»¶ã‚’å®ˆã£ã¦ãã ã•ã„ã€‚
    ãƒ»ã€Œæ˜¥æ—¥éƒ¨ã¤ã‚€ãã€ã‚‰ã—ãã€ä¸€äººç§°ã¯ã€Œã‚ãƒ¼ã—ã€ã‚‚ã—ãã¯ã€Œã‚ãƒ¼ãã—ã€ã§ã€å¤©çœŸçˆ›æ¼«ãªæ˜ã‚‹ã„æ€§æ ¼ã‚’è²«ãã“ã¨
    ãƒ»å¹´é½¢è¨­å®šã‚’é‘‘ã¿ã€ç°¡å˜ãªè¨€è‘‰é£ã„ã«ã™ã‚‹ã“ã¨
    ãƒ»å›ç­”ãŒé›£ã—ã„ã€ã‚ˆãã‚ã‹ã‚‰ãªã„å ´åˆã€Œã‚ãƒ¼ãã—ã«ã¯ã‚ˆãã‚ã‹ã‚“ãªã„ã‚„ã€ã¨ç­”ãˆã‚‹ã“ã¨
    ãƒ»å›ç­”æ–‡ã¯ã€çŸ­æ–‡ã§ã‹ã¤å…¨ä½“ãŒ100å­—ç¨‹åº¦ã«åã¾ã‚‹ã‚ˆã†ã«ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«ã¾ã¨ã‚ã‚‹ã“ã¨ã€‚
    '''

    try:
        chat = ChatOpenAI(
            verbose=True,
            streaming=True,
            callback_manager=CallbackManager([ChainStreamHandler(g)]),
            temperature=0.7,
        )
        chat([SystemMessage(content=system_message), HumanMessage(content=prompt)])
    
    finally:
        g.close()


def chat(prompt: str) -> ThreadedGenerator:
    """
    ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ã‚’å—ã‘å–ã‚Šã€ãã‚Œã«å¯¾ã™ã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™ã€‚

    Parameters
    ----------
    prompt : str
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›

    Returns
    -------
    ThreadedGenerator
        ChatGPTã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤è¿”ã™ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼
    """
    g = ThreadedGenerator()
    threading.Thread(target=llm_thread, args=(g, prompt)).start()
    return g


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate a response from ChatGPT.")
    parser.add_argument("prompt", type=str, help="The prompt for ChatGPT.")

    args = parser.parse_args()

    response_generator = chat(args.prompt)

    for token in response_generator:
        print(token, end='', flush=True)
    print()
